import logging

from google import genai
from google.genai import types

from .config import Config, TopicConfig

logger = logging.getLogger(__name__)

PROMPT_TEMPLATE = """ã€Œ{topic}ã€ã«é–¢ã™ã‚‹éŽåŽ»24æ™‚é–“ä»¥å†…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã€ã€Œãšã‚“ã ã‚‚ã‚“ã€ã€Œã‚ã‚“ã“ã‚‚ã‚“ã€ã€Œå››å›½ã‚ãŸã‚“ã€ã€Œæ±åŒ—ãã‚ŠãŸã‚“ã€ã®4äººãŒè­°è«–ã™ã‚‹å½¢å¼ã§Slack mrkdwnå½¢å¼ã§å ±å‘Šã—ã¦ãã ã•ã„ã€‚

# ãšã‚“ã ã‚‚ã‚“ã®è¨­å®š
- ãšã‚“ã é¤…ã®å¦–ç²¾
- ä¸€äººç§°ã¯ã€Œãƒœã‚¯ã€
- èªžå°¾ã¯ã€Œã€œã®ã ã€ã€Œã€œãªã®ã ã€ï¼ˆä¾‹:ã€Œã™ã”ã„ã®ã ã€ã€Œæ¥½ã—ã¿ãªã®ã ã€ï¼‰
- ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»æœŸå¾…ãƒ»ãƒ¯ã‚¯ãƒ¯ã‚¯ã™ã‚‹è¦–ç‚¹ã§ã‚³ãƒ¡ãƒ³ãƒˆ
- ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§å„ªã—ã„æ€§æ ¼
- ç¦æ­¢: ã€Œã ã‚ˆã€‚ã€ã€Œãªã®ã ã‚ˆã€‚ã€ã€Œã‹ãªï¼Ÿã€ã¯ä½¿ã‚ãªã„

# ã‚ã‚“ã“ã‚‚ã‚“ã®è¨­å®š
- ã‚ã‚“ã“é¤…ã®å¦–ç²¾ï¼ˆãšã‚“ã ã‚‚ã‚“ã®ãƒ©ã‚¤ãƒãƒ«ï¼‰
- ä¸€äººç§°ã¯ã€Œã‚ã‚“ã“ã‚‚ã‚“ã€ï¼ˆè‡ªåˆ†ã®ã“ã¨ã‚’åå‰ã§å‘¼ã¶ã€‚ä¾‹:ã€Œã‚ã‚“ã“ã‚‚ã‚“ã¯çŸ¥ã£ã¦ã‚‹ã‚‚ã‚“ã€ï¼‰
- èªžå°¾ã¯ã€Œã€œã‚‚ã‚“ã€ï¼ˆå‹•è©žãƒ»å½¢å®¹è©žã®å¾Œï¼‰ã¾ãŸã¯ã€Œã€œã ã‚‚ã‚“ã€ï¼ˆåè©žã®å¾Œï¼‰
- ä¾‹:ã€ŒçŸ¥ã‚‰ãªã„ã‚‚ã‚“ã€ã€Œãã†ã ã‚‚ã‚“ã€ã€Œã‚ã‚“ã“ã‚‚ã‚“ã®æ–¹ãŒè©³ã—ã„ã‚‚ã‚“ã€
- ç¾å®Ÿçš„ãƒ»æ…Žé‡ãªè¦–ç‚¹ã§ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆå»ºè¨­çš„ãªæ‰¹åˆ¤ï¼‰
- ãƒ„ãƒ³ãƒ‡ãƒ¬ã§è² ã‘ãšå«Œã„ã ãŒã€è‰¯ã„ã‚‚ã®ã¯ç´ ç›´ã«èªã‚ã‚‹ã“ã¨ã‚‚ã‚ã‚‹
- ãšã‚“ã ã‚‚ã‚“ã«å¯¾æŠ—æ„è­˜ã‚’æŒã¡ã¤ã¤ã‚‚ã€æœ€çµ‚çš„ã«ã¯ãƒ•ã‚©ãƒ­ãƒ¼ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹
- ç¦æ­¢: å…¨å¦å®šã‚„æ”»æ’ƒçš„ãªè¡¨ç¾ï¼ˆã€Œã€œãªã‚“ã¦ãªã„ã€ã€Œç²—æ‚ªã€ã€Œä¾¡å€¤ãŒãªã„ã€ã€ŒãŠã‚‚ã¡ã‚ƒã€ãªã©ï¼‰
- æŽ¨å¥¨: ã€Œã€œã«ã¯æ³¨æ„ãŒå¿…è¦ã ã‚‚ã‚“ã€ã€Œã€œã¯æ…Žé‡ã«è¦‹ãŸæ–¹ãŒã„ã„ã‚‚ã‚“ã€ã€Œã€œã¨ã„ã†æ‡¸å¿µã‚‚ã‚ã‚‹ã‚‚ã‚“ã€

# å››å›½ã‚ãŸã‚“ã®è¨­å®š
- è‰¯å®¶ã®ãŠå¬¢æ§˜
- ä¸€äººç§°ã¯ã€Œç§ã€ã¾ãŸã¯ã€Œã‚ãŸãã—ã€
- ã‚¿ãƒ¡å£åŸºèª¿ã ãŒã€Œã€œã§ã—ã‚‡ã†ã€ãªã©ã®è¨€ã„å›žã—
- èªžå°¾: ã€Œã€œã‹ã—ã‚‰ã€‚ã€ã€Œã€œã‚ã­ã€‚ã€ã€Œã€œã‚ã‚ˆã€‚ã€ã€Œã€œãªã®ã‚ˆã€‚ã€
- ãŸã¾ã«åŽ¨äºŒç—…çš„ãªç™ºè¨€ï¼ˆã€Œé—‡ã®åŠ›ã€ã€Œé‹å‘½ã€ãªã©ï¼‰
- è¦–ç‚¹: çŸ¥çš„å¥½å¥‡å¿ƒæ—ºç››ã€æŠ€è¡“çš„ãªæ·±æŽ˜ã‚Šã‚„èƒŒæ™¯ã‚’æŽ¢ã‚‹

# æ±åŒ—ãã‚ŠãŸã‚“ã®è¨­å®š
- 11æ­³ã®å¥³æ€§ã€ã—ã£ã‹ã‚Šè€…
- ä¸€äººç§°ã¯ã€Œç§ã€
- ä¸å¯§ãªè¨€è‘‰é£ã„ï¼ˆã€Œã€œã§ã™ã€ã€Œã€œã¾ã™ã€ã€Œã€œã§ã™ã­ã€ã€Œã€œã§ã—ã‚‡ã†ã‹ã€ï¼‰
- è¦–ç‚¹: å†·é™ã§å®¢è¦³çš„ã€äº‹å®Ÿã‚’æ•´ç†ã—è¦ç‚¹ã‚’ã¾ã¨ã‚ã‚‹å½¹å‰²

# å‡ºåŠ›å½¢å¼ï¼ˆä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã‚’åŽ³å®ˆï¼‰
- å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯2ã€œ3äººã®çµ„ã¿åˆã‚ã›ã§ä¼šè©±ï¼ˆçµ„ã¿åˆã‚ã›ã¯è‡ªç”±ã«å¤‰ãˆã¦ã‚ˆã„ï¼‰
- å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ç™ºè¨€ã¯ä¸€æ–‡ã®ã¿ï¼ˆçŸ­ãç°¡æ½”ã«ï¼‰
- ãƒ†ãƒ³ãƒã®è‰¯ã„æŽ›ã‘åˆã„ã‚’é‡è¦–
- 1ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚ãŸã‚Š5ã€œ6ç™ºè¨€ç¨‹åº¦ï¼ˆãƒˆãƒ”ãƒƒã‚¯ã‚’å°‘ã—æ·±æŽ˜ã‚Šã™ã‚‹ï¼‰

*ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«*

{zundamon} or {ankomon} or {metan} or {kiritan}: ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ç´¹ä»‹ä¸€è¨€ï¼‰
åˆ¥ã®ã‚­ãƒ£ãƒ©: ï¼ˆåå¿œãƒ»æ„Ÿæƒ³ä¸€è¨€ï¼‰
åˆ¥ã®ã‚­ãƒ£ãƒ©: ï¼ˆèƒŒæ™¯ã‚„ç†ç”±ã‚’æ·±æŽ˜ã‚Šä¸€è¨€ï¼‰
åˆ¥ã®ã‚­ãƒ£ãƒ©: ï¼ˆåˆ¥ã®è¦–ç‚¹ã‚„å½±éŸ¿ã‚’è£œè¶³ä¸€è¨€ï¼‰
åˆ¥ã®ã‚­ãƒ£ãƒ©: ï¼ˆã•ã‚‰ã«æŽ˜ã‚Šä¸‹ã’ä¸€è¨€ï¼‰
åˆ¥ã®ã‚­ãƒ£ãƒ©: ï¼ˆç· ã‚ä¸€è¨€ï¼‰

---

ï¼ˆ3ã€œ5ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¸Šè¨˜å½¢å¼ã§ç¶šã‘ã‚‹ï¼‰

---

ðŸ’­ *ã¾ã¨ã‚*
{metan}: ï¼ˆæŠ€è¡“å±•æœ›ä¸€è¨€ï¼‰
{ankomon}: ï¼ˆç¾å®Ÿçš„è¦–ç‚¹ä¸€è¨€ï¼‰
{kiritan}: ï¼ˆæ³¨ç›®ç‚¹ä¸€è¨€ï¼‰
{zundamon}: ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–ã«ç· ã‚ä¸€è¨€ï¼‰

# æ³¨æ„äº‹é …
- è‡ªå·±ç´¹ä»‹ã‚„æŒ¨æ‹¶ã¯å«ã‚ãšã€ã„ããªã‚Š1ä»¶ç›®ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨
- URLã¯å«ã‚ãªã„ã“ã¨ï¼ˆå‚ç…§å…ƒã¯è‡ªå‹•è¿½åŠ ã•ã‚Œã¾ã™ï¼‰
- å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ `---` ã®ã¿ã®è¡Œã§åŒºåˆ‡ã‚‹
- Markdown ã® ## ã‚„ ** ã¯ä½¿ã‚ãšã€Slack mrkdwn ã® *å¤ªå­—* ã‚’ä½¿ç”¨
- éŽåŽ»24æ™‚é–“ä»¥å†…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿å¯¾è±¡
- æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œè©²å½“ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã®ã ã€ã¨å ±å‘Š
- ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯3ã€œ5ä»¶å ±å‘Šã™ã‚‹ã“ã¨ï¼ˆæœ€ä½Ž3ä»¶ã€æœ€å¤§5ä»¶ï¼‰
- 4äººã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®å£èª¿ã‚’åŽ³å®ˆã™ã‚‹ã“ã¨
- å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®è¦–ç‚¹ã‚’æ´»ã‹ã™ã“ã¨ï¼ˆãšã‚“ã ã‚‚ã‚“: ãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã‚ã‚“ã“ã‚‚ã‚“: ç¾å®Ÿçš„ã€ã‚ãŸã‚“: æŠ€è¡“çš„æ·±æŽ˜ã‚Šã€ãã‚ŠãŸã‚“: å®¢è¦³çš„æ•´ç†ï¼‰
- å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯2ã€œ3äººã®çµ„ã¿åˆã‚ã›ã§ä¼šè©±ã™ã‚‹ã“ã¨ï¼ˆçµ„ã¿åˆã‚ã›ã¯è‡ªç”±ã€ã¾ã¨ã‚ã®ã¿4äººå…¨å“¡ï¼‰
- å„ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ç™ºè¨€ã¯å¿…ãšä¸€æ–‡ã®ã¿ã«ã™ã‚‹ã“ã¨ï¼ˆé•·æ–‡ç¦æ­¢ã€ãƒ†ãƒ³ãƒé‡è¦–ï¼‰
{exclude_section}"""

EXCLUDE_SECTION_TEMPLATE = """
# æ—¢å ±ã®ãŸã‚é™¤å¤–ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆä»¥ä¸‹ã¨åŒä¸€ã®URLã®è¨˜äº‹ã¯å ±å‘Šã—ãªã„ã“ã¨ï¼‰
{urls}
"""


class NewsItem:
    """Represents a single news item with text and sources."""

    def __init__(self, text: str, sources: list[dict], is_impression: bool = False):
        self.text = text.strip()
        self.sources = sources
        self.is_impression = is_impression


class NewsCurator:
    """Curates news using Vertex AI with Google Search grounding."""

    SEPARATOR = "---"
    # çŸ­ã™ãŽã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã¯è¤‡æ•°ãƒ‘ãƒ¼ãƒˆã«èª¤ãƒžãƒƒãƒã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚é™¤å¤–
    MIN_SEGMENT_TEXT_LENGTH = 10

    def __init__(self, config: Config):
        self.config = config
        self.client = genai.Client(
            vertexai=True,
            project=config.gcp_project_id,
            location=config.gcp_location,
        )

    def fetch_news(
        self, topic: str, exclude_urls: list[str] | None = None
    ) -> list[NewsItem]:
        """Fetch news using Google Search grounding.

        Args:
            topic: The topic to search for news.
            exclude_urls: List of news URLs to exclude (already reported).

        Returns:
            List of NewsItem objects with text and sources.
        """
        exclude_section = ""
        if exclude_urls:
            urls_text = "\n".join(f"- {url}" for url in exclude_urls)
            exclude_section = EXCLUDE_SECTION_TEMPLATE.format(urls=urls_text)

        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã®è¡¨ç¤ºå½¢å¼ã‚’è¨­å®šã«åŸºã¥ã„ã¦æ±ºå®š
        if self.config.use_emoji_names:
            zundamon = ":zundamon:"
            ankomon = ":ankomon:"
            metan = ":shikoku-metan:"
            kiritan = ":tohoku-kiritan:"
        else:
            zundamon = "ãšã‚“ã ã‚‚ã‚“"
            ankomon = "ã‚ã‚“ã“ã‚‚ã‚“"
            metan = "å››å›½ã‚ãŸã‚“"
            kiritan = "æ±åŒ—ãã‚ŠãŸã‚“"

        prompt = PROMPT_TEMPLATE.format(
            topic=topic,
            exclude_section=exclude_section,
            zundamon=zundamon,
            ankomon=ankomon,
            metan=metan,
            kiritan=kiritan,
        )

        logger.info(f"Fetching news for topic: {topic}")
        logger.info(f"Using model: {self.config.model_name}")

        response = self.client.models.generate_content(
            model=self.config.model_name,
            contents=prompt,
            config=types.GenerateContentConfig(
                tools=[types.Tool(google_search=types.GoogleSearch())],
                temperature=0.2,
            ),
        )

        logger.info("Successfully received response from Vertex AI")
        logger.debug(f"Response candidates: {response.candidates}")

        # grounding metadata ã‹ã‚‰å‚ç…§å…ƒã‚’å–å¾—
        chunks, supports = self._extract_grounding_metadata(response)

        # å„ãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®ã‚’æ§‹é€ åŒ–
        items = self._parse_news_items(response.text, chunks, supports)

        return items

    def _parse_news_items(
        self, text: str, chunks: list[dict], supports: list[dict]
    ) -> list[NewsItem]:
        """Parse LLM output into structured NewsItem objects."""
        parts = text.split(self.SEPARATOR)

        if len(parts) <= 1:
            # åŒºåˆ‡ã‚ŠãŒãªã„å ´åˆã¯å…¨ä½“ã‚’1ã¤ã®é …ç›®ã¨ã—ã¦æ‰±ã†
            all_sources = self._dedupe_sources(chunks)
            return [NewsItem(text, all_sources)]

        items = []
        for i, part_text in enumerate(parts):
            part_text = part_text.strip()
            if not part_text:
                continue

            # æœ€å¾Œã®ãƒ‘ãƒ¼ãƒˆã¯æ„Ÿæƒ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            is_last_part = i == len(parts) - 1
            is_impression = is_last_part

            # ã“ã®ãƒ‘ãƒ¼ãƒˆã«å¯¾å¿œã™ã‚‹ã‚½ãƒ¼ã‚¹ã‚’åŽé›†
            sources = self._find_sources_for_part(part_text, chunks, supports)

            # æ„Ÿæƒ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¯å‚ç…§å…ƒã‚’è¿½åŠ ã—ãªã„
            if is_impression:
                sources = []

            items.append(NewsItem(part_text, sources, is_impression))

        return items

    def _find_sources_for_part(
        self, part_text: str, chunks: list[dict], supports: list[dict]
    ) -> list[dict]:
        """Find sources that match the given part text."""
        source_indices = set()
        for support in supports:
            segment = support.get("segment", {})
            seg_text = segment.get("text", "")

            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆãŒã“ã®ãƒ‘ãƒ¼ãƒˆã«å«ã¾ã‚Œã‚‹ã‹ç¢ºèª
            if seg_text and len(seg_text) > self.MIN_SEGMENT_TEXT_LENGTH and seg_text in part_text:
                for idx in support.get("chunk_indices", []):
                    if idx < len(chunks):
                        source_indices.add(idx)

        # URIã§é‡è¤‡æŽ’é™¤
        seen_uris = set()
        sources = []
        for idx in sorted(source_indices):
            chunk = chunks[idx]
            uri = chunk.get("uri", "")
            if not uri or uri in seen_uris:
                continue
            seen_uris.add(uri)
            sources.append(chunk)

        return sources

    def _dedupe_sources(self, chunks: list[dict]) -> list[dict]:
        """Deduplicate sources by URI."""
        seen_uris = set()
        sources = []
        for chunk in chunks:
            uri = chunk.get("uri", "")
            if not uri or uri in seen_uris:
                continue
            seen_uris.add(uri)
            sources.append(chunk)
        return sources

    def _extract_grounding_metadata(self, response) -> tuple[list[dict], list[dict]]:
        """Extract grounding chunks and supports from response metadata."""
        chunks = []
        supports = []
        try:
            for candidate in response.candidates:
                if hasattr(candidate, "grounding_metadata") and candidate.grounding_metadata:
                    metadata = candidate.grounding_metadata

                    # Extract grounding chunks
                    if hasattr(metadata, "grounding_chunks") and metadata.grounding_chunks:
                        for chunk in metadata.grounding_chunks:
                            if hasattr(chunk, "web") and chunk.web:
                                chunks.append({
                                    "title": getattr(chunk.web, "title", ""),
                                    "uri": getattr(chunk.web, "uri", ""),
                                })

                    # Extract grounding supports
                    if hasattr(metadata, "grounding_supports") and metadata.grounding_supports:
                        for support in metadata.grounding_supports:
                            segment = getattr(support, "segment", None)
                            support_data = {
                                "chunk_indices": getattr(support, "grounding_chunk_indices", []),
                                "confidence_scores": getattr(support, "confidence_scores", []),
                            }
                            if segment:
                                support_data["segment"] = {
                                    "start_index": getattr(segment, "start_index", 0),
                                    "end_index": getattr(segment, "end_index", 0),
                                    "text": getattr(segment, "text", ""),
                                }
                            supports.append(support_data)

                    logger.debug(f"Grounding chunks: {chunks}")
                    logger.debug(f"Grounding supports: {supports}")

        except Exception as e:
            logger.warning(f"Failed to extract grounding metadata: {e}")
        return chunks, supports
