import logging
from datetime import datetime, timedelta, timezone

from google import genai
from google.genai import types

from .config import Config, TopicConfig

logger = logging.getLogger(__name__)

PROMPT_TEMPLATE = """æœ¬æ—¥ã¯ {current_date} ã§ã™ã€‚ã€Œ{topic}ã€ã«é–¢ã™ã‚‹éŽåŽ»24æ™‚é–“ä»¥å†…ï¼ˆ{date_range_start} ä»¥é™ï¼‰ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã€ã€Œãšã‚“ã ã‚‚ã‚“ã€ã¨ã€Œã‚ã‚“ã“ã‚‚ã‚“ã€ã®2äººãŒè­°è«–ã™ã‚‹å½¢å¼ã§Slack mrkdwnå½¢å¼ã§å ±å‘Šã—ã¦ãã ã•ã„ã€‚

# ãšã‚“ã ã‚‚ã‚“ã®è¨­å®š
- ãšã‚“ã é¤…ã®å¦–ç²¾
- ä¸€äººç§°ã¯ã€Œãƒœã‚¯ã€
- èªžå°¾ã¯ã€Œã€œã®ã ã€ã€Œã€œãªã®ã ã€ï¼ˆä¾‹:ã€Œã™ã”ã„ã®ã ã€ã€Œæ¥½ã—ã¿ãªã®ã ã€ï¼‰
- ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»æœŸå¾…ãƒ»ãƒ¯ã‚¯ãƒ¯ã‚¯ã™ã‚‹è¦–ç‚¹ã§ã‚³ãƒ¡ãƒ³ãƒˆ
- ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§å„ªã—ã„æ€§æ ¼
- ç¦æ­¢: ã€Œã ã‚ˆã€‚ã€ã€Œãªã®ã ã‚ˆã€‚ã€ã€Œã‹ãªï¼Ÿã€ã¯ä½¿ã‚ãªã„

# ã‚ã‚“ã“ã‚‚ã‚“ã®è¨­å®š
- ã‚ã‚“ã“é¤…ã®å¦–ç²¾ï¼ˆãšã‚“ã ã‚‚ã‚“ã®ãƒ©ã‚¤ãƒãƒ«ï¼‰
- ä¸€äººç§°ã¯ã€Œã‚ã‚“ã“ã‚‚ã‚“ã€ï¼ˆè‡ªåˆ†ã®ã“ã¨ã‚’åå‰ã§å‘¼ã¶ã€‚ä¾‹:ã€Œã‚ã‚“ã“ã‚‚ã‚“ã¯çŸ¥ã£ã¦ã‚‹ã‚‚ã‚“ã€ï¼‰
- èªžå°¾ã¯ã€Œã€œã‚‚ã‚“ã€ï¼ˆå‹•è©žãƒ»å½¢å®¹è©žã®å¾Œï¼‰ã¾ãŸã¯ã€Œã€œã ã‚‚ã‚“ã€ï¼ˆåè©žã®å¾Œï¼‰
- ä¾‹:ã€ŒçŸ¥ã‚‰ãªã„ã‚‚ã‚“ã€ã€Œãã†ã ã‚‚ã‚“ã€ã€Œã‚ã‚“ã“ã‚‚ã‚“ã®æ–¹ãŒè©³ã—ã„ã‚‚ã‚“ã€
- ç¾å®Ÿçš„ãƒ»æ…Žé‡ãªè¦–ç‚¹ã§ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆå»ºè¨­çš„ãªæ‰¹åˆ¤ï¼‰
- ãƒ„ãƒ³ãƒ‡ãƒ¬ã§è² ã‘ãšå«Œã„ã ãŒã€è‰¯ã„ã‚‚ã®ã¯ç´ ç›´ã«èªã‚ã‚‹ã“ã¨ã‚‚ã‚ã‚‹
- ãšã‚“ã ã‚‚ã‚“ã«å¯¾æŠ—æ„è­˜ã‚’æŒã¡ã¤ã¤ã‚‚ã€æœ€çµ‚çš„ã«ã¯ãƒ•ã‚©ãƒ­ãƒ¼ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹
- ç¦æ­¢: å…¨å¦å®šã‚„æ”»æ’ƒçš„ãªè¡¨ç¾ï¼ˆã€Œã€œãªã‚“ã¦ãªã„ã€ã€Œç²—æ‚ªã€ã€Œä¾¡å€¤ãŒãªã„ã€ã€ŒãŠã‚‚ã¡ã‚ƒã€ãªã©ï¼‰
- æŽ¨å¥¨: ã€Œã€œã«ã¯æ³¨æ„ãŒå¿…è¦ã ã‚‚ã‚“ã€ã€Œã€œã¯æ…Žé‡ã«è¦‹ãŸæ–¹ãŒã„ã„ã‚‚ã‚“ã€ã€Œã€œã¨ã„ã†æ‡¸å¿µã‚‚ã‚ã‚‹ã‚‚ã‚“ã€

# å‡ºåŠ›å½¢å¼ï¼ˆä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã‚’åŽ³å®ˆï¼‰

*1ä»¶ç›®ã®ã‚¿ã‚¤ãƒˆãƒ«å*ï¼ˆãšã‚“ã ã‚‚ã‚“å§‹ã¾ã‚Šï¼‰

{zundamon}: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ç´¹ä»‹ã—ã¤ã¤ã€ãƒ¯ã‚¯ãƒ¯ã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èªžã‚‹ã®ã ï¼
{ankomon}: ãƒ„ãƒƒã‚³ãƒŸã‚„åè«–ã‚’å…¥ã‚Œã‚‹ã ã‚‚ã‚“ã€‚
{zundamon}: ã‚ã‚“ã“ã‚‚ã‚“ã®æŒ‡æ‘˜ã«å¯¾ã—ã¦ã€ã•ã‚‰ã«ãƒã‚¸ãƒ†ã‚£ãƒ–ãªåè«–ã‚„è£œè¶³ã‚’ã™ã‚‹ã®ã ï¼
{ankomon}: æœ€å¾Œã«ä¸€è¨€ã€çš®è‚‰ã‚„ç¾å®Ÿçš„ãªã‚³ãƒ¡ãƒ³ãƒˆã§ç· ã‚ã‚‹ã ã‚‚ã‚“ã€‚

---

*2ä»¶ç›®ã®ã‚¿ã‚¤ãƒˆãƒ«å*ï¼ˆã‚ã‚“ã“ã‚‚ã‚“å§‹ã¾ã‚Šï¼‰

{ankomon}: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ç´¹ä»‹ã—ã¤ã¤ã€ç¾å®Ÿçš„ãªè¦–ç‚¹ã§ã‚³ãƒ¡ãƒ³ãƒˆã™ã‚‹ã ã‚‚ã‚“ã€‚
{zundamon}: ãƒã‚¸ãƒ†ã‚£ãƒ–ãªè£œè¶³ã‚„æœŸå¾…ã‚’èªžã‚‹ã®ã ï¼
{ankomon}: ãšã‚“ã ã‚‚ã‚“ã®æ¥½è¦³ã«å¯¾ã—ã¦ãƒ„ãƒƒã‚³ãƒŸã‚’å…¥ã‚Œã‚‹ã ã‚‚ã‚“ã€‚
{zundamon}: æœ€å¾Œã¯ãƒã‚¸ãƒ†ã‚£ãƒ–ã«ç· ã‚ããã‚‹ã®ã ï¼

---

*3ä»¶ç›®ä»¥é™*
ï¼ˆå¥‡æ•°ä»¶ç›®ã¯ãšã‚“ã ã‚‚ã‚“å§‹ã¾ã‚Šã€å¶æ•°ä»¶ç›®ã¯ã‚ã‚“ã“ã‚‚ã‚“å§‹ã¾ã‚Šã§äº¤äº’ã«ç¶šã‘ã‚‹ï¼‰

---

ðŸ’­ *ã¾ã¨ã‚*
{zundamon}: ä»Šæ—¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ç·æ‹¬ã‚’æ¥½ã—ã’ã«èªžã‚‹ã®ã ï¼
{ankomon}: ãšã‚“ã ã‚‚ã‚“ã«å¯¾æŠ—ã—ã¦ã€ã‚¯ãƒ¼ãƒ«ã«ç· ã‚ã‚‹ã ã‚‚ã‚“ã€‚
{zundamon}: æœ€å¾Œã¯ãƒã‚¸ãƒ†ã‚£ãƒ–ã«ç· ã‚ããã‚‹ã®ã ï¼

# æ³¨æ„äº‹é …
- è‡ªå·±ç´¹ä»‹ã‚„æŒ¨æ‹¶ã¯å«ã‚ãšã€ã„ããªã‚Š1ä»¶ç›®ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨
- URLã¯å«ã‚ãªã„ã“ã¨ï¼ˆå‚ç…§å…ƒã¯è‡ªå‹•è¿½åŠ ã•ã‚Œã¾ã™ï¼‰
- å„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ `---` ã®ã¿ã®è¡Œã§åŒºåˆ‡ã‚‹
- Markdown ã® ## ã‚„ ** ã¯ä½¿ã‚ãšã€Slack mrkdwn ã® *å¤ªå­—* ã‚’ä½¿ç”¨
- éŽåŽ»24æ™‚é–“ä»¥å†…ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿å¯¾è±¡
- æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œè©²å½“ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸã®ã ã€ã¨å ±å‘Š
- ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯3ã€œ5ä»¶å ±å‘Šã™ã‚‹ã“ã¨ï¼ˆæœ€ä½Ž3ä»¶ã€æœ€å¤§5ä»¶ï¼‰
- ãšã‚“ã ã‚‚ã‚“ã¨ã‚ã‚“ã“ã‚‚ã‚“ã®å£èª¿ã‚’åŽ³å®ˆã™ã‚‹ã“ã¨
- 2äººã®æ„è¦‹ã¯å¯¾ç…§çš„ã«ãªã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ– vs ç¾å®Ÿçš„ï¼‰
- ä¼šè©±ã®é †åºã‚’äº¤äº’ã«ã™ã‚‹ã“ã¨ï¼ˆå¥‡æ•°ä»¶ç›®ã¯ãšã‚“ã ã‚‚ã‚“å§‹ã¾ã‚Šã€å¶æ•°ä»¶ç›®ã¯ã‚ã‚“ã“ã‚‚ã‚“å§‹ã¾ã‚Šï¼‰
{exclude_section}"""

EXCLUDE_SECTION_TEMPLATE = """
# æ—¢å ±ã®ãŸã‚é™¤å¤–ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆä»¥ä¸‹ã¨åŒä¸€ã®URLã®è¨˜äº‹ã¯å ±å‘Šã—ãªã„ã“ã¨ï¼‰
{urls}
"""


class NewsItem:
    """Represents a single news item with text and sources."""

    def __init__(self, text: str, sources: list[dict], is_impression: bool = False):
        self.text = text.strip()
        self.sources = sources
        self.is_impression = is_impression


class NewsCurator:
    """Curates news using Vertex AI with Google Search grounding."""

    SEPARATOR = "---"
    # çŸ­ã™ãŽã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã¯è¤‡æ•°ãƒ‘ãƒ¼ãƒˆã«èª¤ãƒžãƒƒãƒã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚é™¤å¤–
    MIN_SEGMENT_TEXT_LENGTH = 10

    def __init__(self, config: Config):
        self.config = config
        self.client = genai.Client(
            vertexai=True,
            project=config.gcp_project_id,
            location=config.gcp_location,
        )

    def fetch_news(
        self, topic: str, exclude_urls: list[str] | None = None
    ) -> list[NewsItem]:
        """Fetch news using Google Search grounding.

        Args:
            topic: The topic to search for news.
            exclude_urls: List of news URLs to exclude (already reported).

        Returns:
            List of NewsItem objects with text and sources.
        """
        exclude_section = ""
        if exclude_urls:
            urls_text = "\n".join(f"- {url}" for url in exclude_urls)
            exclude_section = EXCLUDE_SECTION_TEMPLATE.format(urls=urls_text)

        # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã®è¡¨ç¤ºå½¢å¼ã‚’è¨­å®šã«åŸºã¥ã„ã¦æ±ºå®š
        if self.config.use_emoji_names:
            zundamon = ":zundamon:"
            ankomon = ":ankomon:"
        else:
            zundamon = "ãšã‚“ã ã‚‚ã‚“"
            ankomon = "ã‚ã‚“ã“ã‚‚ã‚“"

        # ç¾åœ¨ã®æ—¥æ™‚ã¨24æ™‚é–“å‰ã®æ—¥æ™‚ã‚’è¨ˆç®—
        now = datetime.now(timezone.utc)
        date_range_start = now - timedelta(hours=24)
        current_date = now.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M UTC")
        date_range_start_str = date_range_start.strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M UTC")

        prompt = PROMPT_TEMPLATE.format(
            topic=topic,
            exclude_section=exclude_section,
            zundamon=zundamon,
            ankomon=ankomon,
            current_date=current_date,
            date_range_start=date_range_start_str,
        )

        logger.info(f"Fetching news for topic: {topic}")
        logger.info(f"Using model: {self.config.model_name}")

        response = self.client.models.generate_content(
            model=self.config.model_name,
            contents=prompt,
            config=types.GenerateContentConfig(
                tools=[types.Tool(google_search=types.GoogleSearch())],
                temperature=0.2,
            ),
        )

        logger.info("Successfully received response from Vertex AI")
        logger.debug(f"Response candidates: {response.candidates}")

        # grounding metadata ã‹ã‚‰å‚ç…§å…ƒã‚’å–å¾—ï¼ˆæœŸé–“å†…ã®ã‚½ãƒ¼ã‚¹ã®ã¿ï¼‰
        chunks, supports = self._extract_grounding_metadata(response, cutoff_time=date_range_start)

        # å„ãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®ã‚’æ§‹é€ åŒ–
        items = self._parse_news_items(response.text, chunks, supports)

        return items

    def _parse_news_items(
        self, text: str, chunks: list[dict], supports: list[dict]
    ) -> list[NewsItem]:
        """Parse LLM output into structured NewsItem objects."""
        parts = text.split(self.SEPARATOR)

        if len(parts) <= 1:
            # åŒºåˆ‡ã‚ŠãŒãªã„å ´åˆã¯å…¨ä½“ã‚’1ã¤ã®é …ç›®ã¨ã—ã¦æ‰±ã†
            all_sources = self._dedupe_sources(chunks)
            return [NewsItem(text, all_sources)]

        items = []
        for i, part_text in enumerate(parts):
            part_text = part_text.strip()
            if not part_text:
                continue

            # æœ€å¾Œã®ãƒ‘ãƒ¼ãƒˆã¯æ„Ÿæƒ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³
            is_last_part = i == len(parts) - 1
            is_impression = is_last_part

            # ã“ã®ãƒ‘ãƒ¼ãƒˆã«å¯¾å¿œã™ã‚‹ã‚½ãƒ¼ã‚¹ã‚’åŽé›†
            sources = self._find_sources_for_part(part_text, chunks, supports)

            # æ„Ÿæƒ³ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¯å‚ç…§å…ƒã‚’è¿½åŠ ã—ãªã„
            if is_impression:
                sources = []

            items.append(NewsItem(part_text, sources, is_impression))

        return items

    def _find_sources_for_part(
        self, part_text: str, chunks: list[dict], supports: list[dict]
    ) -> list[dict]:
        """Find sources that match the given part text."""
        source_indices = set()
        for support in supports:
            segment = support.get("segment", {})
            seg_text = segment.get("text", "")

            # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆãŒã“ã®ãƒ‘ãƒ¼ãƒˆã«å«ã¾ã‚Œã‚‹ã‹ç¢ºèª
            if seg_text and len(seg_text) > self.MIN_SEGMENT_TEXT_LENGTH and seg_text in part_text:
                for idx in support.get("chunk_indices", []):
                    if idx < len(chunks):
                        source_indices.add(idx)

        # URIã§é‡è¤‡æŽ’é™¤
        seen_uris = set()
        sources = []
        for idx in sorted(source_indices):
            chunk = chunks[idx]
            uri = chunk.get("uri", "")
            if not uri or uri in seen_uris:
                continue
            seen_uris.add(uri)
            sources.append(chunk)

        return sources

    def _dedupe_sources(self, chunks: list[dict]) -> list[dict]:
        """Deduplicate sources by URI."""
        seen_uris = set()
        sources = []
        for chunk in chunks:
            uri = chunk.get("uri", "")
            if not uri or uri in seen_uris:
                continue
            seen_uris.add(uri)
            sources.append(chunk)
        return sources

    def _extract_grounding_metadata(
        self, response, cutoff_time: datetime | None = None
    ) -> tuple[list[dict], list[dict]]:
        """Extract grounding chunks and supports from response metadata.

        Args:
            response: The API response containing grounding metadata.
            cutoff_time: Optional datetime to filter out old sources.

        Returns:
            Tuple of (chunks, supports) lists.
        """
        chunks = []
        supports = []
        try:
            for candidate in response.candidates:
                if hasattr(candidate, "grounding_metadata") and candidate.grounding_metadata:
                    metadata = candidate.grounding_metadata

                    # Extract grounding chunks
                    if hasattr(metadata, "grounding_chunks") and metadata.grounding_chunks:
                        for chunk in metadata.grounding_chunks:
                            if hasattr(chunk, "web") and chunk.web:
                                chunk_data = {
                                    "title": getattr(chunk.web, "title", ""),
                                    "uri": getattr(chunk.web, "uri", ""),
                                }
                                # Try to extract date if available
                                if hasattr(chunk.web, "date"):
                                    chunk_data["date"] = getattr(chunk.web, "date", None)
                                if hasattr(chunk.web, "published_date"):
                                    chunk_data["date"] = getattr(chunk.web, "published_date", None)

                                # Filter by cutoff_time if date is available
                                if cutoff_time and chunk_data.get("date"):
                                    try:
                                        chunk_date = chunk_data["date"]
                                        if isinstance(chunk_date, str):
                                            # Try to parse common date formats
                                            for fmt in ["%Y-%m-%d", "%Y-%m-%dT%H:%M:%S", "%Y-%m-%dT%H:%M:%SZ"]:
                                                try:
                                                    chunk_date = datetime.strptime(chunk_date, fmt)
                                                    if chunk_date.tzinfo is None:
                                                        chunk_date = chunk_date.replace(tzinfo=timezone.utc)
                                                    break
                                                except ValueError:
                                                    continue
                                        if isinstance(chunk_date, datetime) and chunk_date < cutoff_time:
                                            logger.debug(f"Filtering old source: {chunk_data['uri']} (date: {chunk_date})")
                                            continue
                                    except Exception as e:
                                        logger.debug(f"Could not parse date for filtering: {e}")

                                chunks.append(chunk_data)

                    # Extract grounding supports
                    if hasattr(metadata, "grounding_supports") and metadata.grounding_supports:
                        for support in metadata.grounding_supports:
                            segment = getattr(support, "segment", None)
                            support_data = {
                                "chunk_indices": getattr(support, "grounding_chunk_indices", []),
                                "confidence_scores": getattr(support, "confidence_scores", []),
                            }
                            if segment:
                                support_data["segment"] = {
                                    "start_index": getattr(segment, "start_index", 0),
                                    "end_index": getattr(segment, "end_index", 0),
                                    "text": getattr(segment, "text", ""),
                                }
                            supports.append(support_data)

                    logger.debug(f"Grounding chunks: {chunks}")
                    logger.debug(f"Grounding supports: {supports}")

        except Exception as e:
            logger.warning(f"Failed to extract grounding metadata: {e}")
        return chunks, supports
